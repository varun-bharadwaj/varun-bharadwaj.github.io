<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>VROOM: Visual Reconstruction over Onboard Multiview</title>
  <link href="./files/style.css" rel="stylesheet">
  <script type="text/javascript" src="./files/jquery.mlens-1.0.min.js"></script>
  <script type="text/javascript" src="./files/jquery.js"></script>
  <style>
    .divider {
      border-right: 2px dashed #737373;
      width: 2px;
    }
  </style>
  <style>
    .divider_horizontal {
      border-top: 2px dashed #737373;
      display: block;
      width: 100%;
      margin: 10px 0;
    }
  </style>
  <!-- Begin new styles from demos page -->
  <style>
    body {
      padding: 2em;
      font-family: sans-serif;
    }

    iframe {
      border-radius: 0.5em;
      width: 27.5em;
      height: 27.5em;
      border: none;
      box-shadow: 0 0 1em 0em rgba(0, 0, 0, 0.15);
    }

    @media (max-width: 768px) {
      iframe {
        width: 100%;
        height: 30em;
      }
    }

    a,
    a:link {
      color: #777;
    }

    /* Styles for the instruction icons and text */
    .instructions {
      display: flex;
      justify-content: center;
      align-items: center;
      flex-wrap: wrap;
      gap: 1.2em;
      text-align: center;
      padding: 0.6em;
    }

    .instruction-item {
      display: flex;
      align-items: center;
      gap: 0.5em;
      font-size: 1.2em;
    }

    .instruction-item img {
      width: 36px;
      height: 36px;
    }

    .instructions_small {
      display: flex;
      justify-content: center;
      align-items: center;
      flex-wrap: wrap;
      gap: 1em;
      text-align: center;
      padding: 0.5em;
    }

    .instruction-item_small {
      display: flex;
      align-items: center;
      gap: 0.3em;
      font-size: 1.0em;
    }

    .instruction-item_small img {
      width: 30px;
      height: 30px;
    }
  </style>
  <!-- End new styles -->
</head>

<body>
  <div class="content">
    <h1 style="line-height: 0.75;">
      <strong>VROOM: Visual Reconstruction over Onboard Multiview</strong>
    </h1>
    <p id="authors">
      <span>
        <a>Yajat Yadav<sup>+</sup></a>
      </span>
      <span>
        <a>Varun Bharadwaj<sup>+</sup></a>
      </span>
      <span>
        <a>Tanish Baranwal<sup>+</sup></a>
      </span>
      <br>
      <span>
        <a>Jathin Korrapati<sup>+</sup></a>
      </span>

      <br>
      <span class="institution"><a href="https://bair.berkeley.edu/"><sup>1</sup> UC Berkeley</a>
        <span class="institution"></span>
        <span class="institution">(+: equal contribution)</span>
        <br>
        <br>
        <br>
    </p>
    </p>

    <!-- <br>
        <img src="./files/teaser_monst3r.png" class="teaser-gif" style="width:100%;"><br> -->

    <!-- add video files/teaser_vid_v2_lowres.mp4 -->
    <video width="100%" controls>
      <source src="vroom_demo_video.mp4" type="video/mp4">
    </video>
    <a> A sample segment of the race, and the corresponding track reconstruction + car motion recovered by our
      method.</a>
    <br>
    <br>

    <a style="text-align:center">
      <font size="+1">
        We introduce <strong>VROOM</strong>, a system for reconstructing 3D models of Formula 1
        circuits using
        only onboard camera footage from racecars. Leveraging video data from the 2023 Monaco Grand Prix, we address
        challenges in the videos such as high-speed motion, sharp turns of perspectives from a singular camera
        perspective.
      </font>
    </a>
    <font size="+2">
      <p style="text-align: center;">
        <a href="files\VROOM.pdf" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/yajatyadav/vroom" target="_blank">[Code]</a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="files/bibtex.txt" target="_blank">[BibTeX]</a>
      </p>
    </font>
  </div>

  <!-- Begin new content: Demos -->
  <div class="content">
    <h2>Interactive 4D Visualization</h2>
    <p>Explore our 4D reconstruction results of VROOM on various track mini-sectors.
      <!-- Instructions with icons -->
    <div class="instructions" style="margin-top: -1em;">
      <div class="instruction-item" style="display: flex; align-items: center;">
        <img src="icons/left-click.png" alt="Left Click" style="margin-right: 0px;">
        <span>Drag with <em>left</em> click to <strong>rotate</strong> view</span>
      </div>

      <div class="instruction-item" style="display: flex; align-items: center; margin-top: 10px;">
        <img src="icons/scroll-wheel.png" alt="Scroll Wheel" style="margin-right: -5px;">
        <span>Scroll to <strong>zoom</strong> in/out</span>
      </div>

      <div class="instruction-item" style="display: flex; align-items: center; margin-top: 10px;">
        <img src="icons/right-click.png" alt="Right Click" style="margin-right: 0px;">
        <span>Drag with <em>right</em> click to <strong>move</strong> view</span>
      </div>
    </div>

    <div class="instructions_small">
      <div class="instruction-item_small" style="display: flex; align-items: center; margin-top: -15px;">
        <img src="icons/letter-w.png" alt="W" style="margin-right: -10px;">
        <img src="icons/letter-s.png" alt="S" style="margin-right: 0px;">
        <span>Moving forward and backward</span>
      </div>

      <div class="instruction-item_small" style="display: flex; align-items: center; margin-top: -15px;">
        <img src="icons/letter-a.png" alt="A" style="margin-right: -10px;">
        <img src="icons/letter-d.png" alt="D" style="margin-right: 0px;">
        <span>Moving left and right</span>
      </div>

      <div class="instruction-item_small" style="display: flex; align-items: center; margin-top: -15px;">
        <img src="icons/letter-q.png" alt="Q" style="margin-right: -10px;">
        <img src="icons/letter-e.png" alt="E" style="margin-right: 0px;">
        <span>Moving upward and downward</span>
      </div>
    </div>

    <!-- Wrapper for iframes -->
    <div id="wrapper" style="
        display: flex;
        flex-wrap: wrap;
        justify-content: center;
        align-items: center;
        gap: 2em;
      ">
      <!-- Insert two iframes -->


      <iframe
        src="https://monst3r-project.github.io/build/?playbackPath=https://varun-bharadwaj.github.io/vroom/recordings/recording_test_out.viser"></iframe>

      <iframe
        src="https://monst3r-project.github.io/build/?playbackPath=https://varun-bharadwaj.github.io/vroom/recordings/recording_test_out_2.viser"></iframe>

    </div>

  </div>

  <!-- End new content -->

  <div class="content">
    <h2 style="text-align:center;">Abstract</h2>
    <font size="-0.">
      <p>We introduce VROOM, a system for reconstructing 3D
        models of Formula 1 circuits using only onboard camera
        footage from racecars. Leveraging video data from the
        2023 Monaco Grand Prix, we address challenges in the
        videos such as high-speed motion, sharp turns of perspec-
        tives from a singular camera perspective. Our pipeline uti-
        lizes different methods such as DROIDâ€”Slam, AnyCam,
        and Monst3r and combine preprocessing techniques such
        as different methods of masking, temporal chunking, and
        resolution scaling to account for dynamic motion and com-
        putational constraints. We show Vroom is able to partially
        recover the track and vehicle trajectories in complex envi-
        ronments. These findings indicate the feasibility of using
        onboard video for scalable 4D reconstruction across multi-
        ple agents in real-world settings.
      </p>
    </font>
  </div>

  <div class="content">
    <h2>Preprocessing Methods</h2>
    <p> In order to make our method with the long F1 videos, we implement several preprocessing methods to improve
      efficiency while maintaining reconstruction quality.
    <h3>
      <center>Masking</center>
    </h3>
    <a> Since the car remains fixed with respect to the camera in the videos, this often interferes with the
      reconstruction. We try multiple masking methods to address this issue.</a>

    <div style="display: flex;">
      <img src="mask_car.png" style="width: 50%;">
      <img src="mask_half.png" style="width: 50%;">
    </div>
    <a>
      <strong>Left</strong>: Masking just the car fails to solve the issue.
      <strong>Right</strong>: Masking bottom half of the frame works best.
    </a>

    <p>Next, in addition to downsampling resolution and FPS, we also strategically chunk the video on straight segments
      in the race. Since our method processes the video chunk by chunk, chunking on the straights is essential for
      ensuring each turn is reconstructed with the highest accuracy possible.</p>
    <video width="100%" controls>
      <source src="chunk_1_video.mp4" type="video/mp4">
    </video>
    <video width="100%" controls>
      <source src="chunk_2_video.mp4" type="video/mp4">
    </video>
    <h3>
      <center>(<em>Top</em>) First Video Chunk; (<em>Bottom</em>) Second Video Chunk. As we see, the two overlap in a
        straight segment of the race.</center>
    </h3>
  </div>

  <div class="content">
    <h2>Chunk-wise Point Cloud and Camera Extrinsics</h2>
    <p> We utilize and extend the monst3r repo for learning the point cloud and camera parameters per chunk. </p>
    <img class="summary-img" src="./files/fig5_system.png" style="width:100%;">
    <h3>
      <center>Dynamic global point cloud and camera pose estimation</center>
    </h3>
    <a>
      This figure is from the MonST3R paper and illustrates the system we used. For each chunk, a window slides across
      the video to build a graph with edges between pairs of frames. For each pair, optical flow is found using an
      off-the-shelf method, and the point cloud found by Monst3r.
      These intermediates are used in a global-level bundle adjustment to optimize the point cloud and camera
      parameters.</a>
    <a>
      After repeating the above process with each chunk, we stitch together the chunks by utilizing the overlapping
      frames- we utilize this correspondence to solve for the transformation matrix between frame i's extrinsics and
      frame (i+1)'s extrinsics. This allows us to stitch together the camera trajectories, as well as combine all the
      point clouds in one global reference frame.
    </a>

    <!-- <p> The global optimization is based on the following three loss terms: </p>
    <ul>
      <li> <strong>Alignment loss</strong>: align the global point cloud of each frame with every pairwise pointmap. </li>
      <li> <strong>Flow loss</strong>: encourage the projected flow from global pointmaps and camera poses to be consitent with the estimated flow. </li>
      <li> <strong>Smoothness loss</strong>: enforce the temporal smoothness of camera trajectory. </li>
    </ul> -->
  </div>

  <div class="content">
    <h2>Results - Turn Reconstructions</h2>
    <table border="1" style="border-collapse: collapse; text-align: center;">
      <caption style="font-weight: bold; margin-bottom: 10px;">Predicted vs Ground Truth Comparisons</caption>
      tr>
      <th>Turn</th>
      <th>Predicted</th>
      <th>Ground Truth</th>
      </tr>
      <tr>
        <td>Turn 1</td>
        <td><img src="turn_one_ours.png" alt=" Turn 1" width="100%"></td>
        <td><img src="air_view_of_turn_one.png" alt="Ground Truth 1" width="100%"></td>
      </tr>
      <tr>
        <td>Turn 8</td>
        <td><img src="our_pred_turn_8.png" alt="Prediction 2" width="100%"></td>
        <td><img src="air_view_of_turn_eight.png" alt="Ground Truth 2" width="100%"></td>
      </tr>
      <tr>
        <td>Failure case; Turn 15 + 16</td>
        <td><img src="our_turn_15_16.png" alt="Prediction 3" width="100%"></td>
        <td><img src="chicane_gt (1).jpg" alt="Ground Truth 3" width="100%"></td>
      </tr>
      <tr>
        <td>Turn 8</td>
        <td><img src="our_turn_18.png" alt="Prediction 4" width="100%"></td>
        <td><img src="turn18_gt (1).jpg" alt="Ground Truth 4" width="100%"></td>
      </tr>
    </table>
    <!-- todo, video depth example from davis -->
  </div>



  <div class="content">
    <h2>BibTex</h2>
    <font size="-0.">
      <code> @article{yadav2024vroom,<br>
  &nbsp;&nbsp;author       = {Yadav, Yajat and Bharadwaj, Varun and Korrapati, Jathin and Baranwal, Tanish},<br>
  &nbsp;&nbsp;title        = {VROOM: Visual Reconstruction over Onboard Multiview},<br>  
  &nbsp;&nbsp;year         = {2025},<br>
  &nbsp;&nbsp;note         = {Unpublished manuscript, for a class project},<br>
  &nbsp;&nbsp;institution  = {University of California, Berkeley},<br>
  &nbsp;&nbsp;url          = {http://varun-bharadwaj.github.io/vroom},<br>
}</code>
    </font>
  </div>

  <div class="content" id="acknowledgements">
    <p><strong>Acknowledgements</strong>:
      We borrow this template from the Monst3r paper at <a href="https://monst3r-project.github.io//">Monst3r</a>.
      Similarly,
      the interactive 4D visualization is inspired by the visualizations presented by Monst3r.
      <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). -->
    </p>
  </div>
</body>

</html>